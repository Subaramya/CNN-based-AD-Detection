{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24267,"status":"ok","timestamp":1686395034111,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"FvJuvqZCfi7d","outputId":"ebfe3c10-5d0a-4ee2-b60e-31a3142990f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47Wy5vEPy4CG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9852,"status":"ok","timestamp":1686395047412,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"Zq68DSY2rP2W","outputId":"48b1ab68-28d5-41bb-f403-e2ea1d3a6783"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras.utils\n","  Downloading keras-utils-1.0.13.tar.gz (2.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from keras.utils) (2.12.0)\n","Building wheels for collected packages: keras.utils\n","  Building wheel for keras.utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras.utils: filename=keras_utils-1.0.13-py3-none-any.whl size=2634 sha256=11890645eb440051b63a5420458d93726670cc4af3437dfc39e4704cd6cac5f3\n","  Stored in directory: /root/.cache/pip/wheels/5c/c0/b3/0c332de4fd71f3733ea6d61697464b7ae4b2b5ff0300e6ca7a\n","Successfully built keras.utils\n","Installing collected packages: keras.utils\n","Successfully installed keras.utils-1.0.13\n"]}],"source":["!pip install keras.utils\n","import keras.utils"]},{"cell_type":"markdown","metadata":{"id":"-TnLMJAhf_6_"},"source":["## Import Packages"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1382,"status":"ok","timestamp":1686395053618,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"4NEVsJ4DgDtS"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, AveragePooling2D, BatchNormalization\n","from keras.applications.vgg16 import VGG16\n","from keras.callbacks import Callback\n","\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":789,"status":"ok","timestamp":1686395060674,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"bHj9E67jZGgv"},"outputs":[],"source":["No_pos= 62;\n","No_neg = 68;\n","pos_samples = np.zeros((No_pos, 80, 80))\n","neg_samples = np.zeros((No_neg, 80, 80))\n","#print(pos_samples)\n","#print(pos_samples[32,:,:].shape)"]},{"cell_type":"markdown","metadata":{"id":"E3SYkwy8hKhF"},"source":["Read Data from file"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45436,"status":"ok","timestamp":1686395106888,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"GshC1_l0AcMf","outputId":"c359d20c-959c-43d9-f360-535924b409f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 007_S_4568\n","1 003_S_4136\n","2 005_S_4707\n","3 003_S_5165\n","4 003_S_4373\n","5 005_S_4910\n","6 005_S_5119\n","7 003_S_4892\n","8 005_S_5038\n","9 003_S_4152\n","10 016_S_5032\n","11 016_S_4887\n","12 021_S_4718\n","13 027_S_4962\n","14 029_S_4307\n","15 027_S_4964\n","16 027_S_4802\n","17 007_S_4911\n","18 021_S_4924\n","19 027_S_4938\n","20 098_S_4201\n","21 127_S_4500\n","22 109_S_4378\n","23 098_S_4215\n","24 094_S_4282\n","25 126_S_4686\n","26 126_S_4494\n","27 052_S_4959\n","28 094_S_4737\n","29 127_S_4749\n","30 127_S_5067\n","31 127_S_5056\n","32 127_S_4940\n","33 003_S_6833\n","34 006_S_6689\n","35 013_S_6768\n","36 016_S_6708\n","37 027_S_6648\n","38 027_S_6733\n","39 027_S_6849\n","40 032_S_6600\n","41 033_S_6705\n","42 033_S_6824\n","43 057_S_6746\n","44 094_S_6736\n","45 114_S_6347\n","46 114_S_6368\n","47 114_S_6595\n","48 116_S_6100\n","49 116_S_6543\n","50 126_S_6683\n","51 127_S_6433\n","52 129_S_6763\n","53 135_S_6284\n","54 135_S_6389\n","55 135_S_6545\n","56 168_S_6735\n","57 168_S_6754\n","58 168_S_6827\n","59 168_S_6828\n","60 168_S_6843\n","61 301_S_6592\n","62 003_S_4441\n","63 003_S_4350\n","64 003_S_4555\n","65 003_S_4644\n","66 003_S_4081\n","67 003_S_4288\n","68 003_S_4119\n","69 029_S_4290\n","70 007_S_4516\n","71 003_S_4839\n","72 003_S_4872\n","73 016_S_4097\n","74 016_S_4952\n","75 007_S_4637\n","76 021_S_4335\n","77 016_S_4951\n","78 021_S_4558\n","79 098_S_4050\n","80 094_S_4649\n","81 098_S_4002\n","82 098_S_4003\n","83 099_S_4076\n","84 098_S_4506\n","85 094_S_4234\n","86 094_S_4459\n","87 098_S_4275\n","88 094_S_4560\n","89 127_S_4843\n","90 099_S_4104\n","91 129_S_4369\n","92 129_S_4422\n","93 127_S_4604\n","94 002_S_0413\n","95 002_S_1261\n","96 002_S_1280\n","97 002_S_4213\n","98 002_S_6007\n","99 002_S_6009\n","100 002_S_6030\n","101 002_S_6053\n","102 002_S_6066\n","103 002_S_6103\n","104 002_S_6456\n","105 003_S_6014\n","106 003_S_6067\n","107 003_S_6257\n","108 003_S_6259\n","109 003_S_6260\n","110 003_S_6490\n","111 003_S_6644\n","112 005_S_0602\n","113 005_S_0610\n","114 005_S_6084\n","115 005_S_6093\n","116 006_S_0498\n","117 006_S_0731\n","118 006_S_4357\n","119 006_S_6209\n","120 006_S_6234\n","121 006_S_6277\n","122 006_S_6375\n","123 009_S_4388\n","124 011_S_6465\n","125 011_S_6714\n","126 013_S_4580\n","127 013_S_6780\n","128 014_S_4401\n","129 014_S_4576\n"]}],"source":["index = 0;\n","for root, dirnames, filenames in os.walk('/content/drive/MyDrive/M.Phil/ADNI2/AD/'):\n","  for dirname in dirnames:\n","    path = ('/content/drive/MyDrive/M.Phil/ADNI2/AD/' + dirname + \"/\" + 'NormAdj_Matrix_WeightedN.csv')\n","    data = pd.read_csv(path, header=None)\n","    pos_samples[index, :, : ] = data\n","    print(index, dirname)\n","    index = index + 1\n","index = 0;\n","for root, dirnames, filenames in os.walk('/content/drive/MyDrive/M.Phil/ADNI2/CN/'):\n","  for dirname in dirnames:\n","    path = ('/content/drive/MyDrive/M.Phil/ADNI2/CN/' + dirname + \"/\" + 'NormAdj_Matrix_WeightedN.csv')\n","    #print(path)\n","    data = pd.read_csv(path, header=None)\n","    neg_samples[index, :, : ] = data\n","    print(index + 62, dirname)\n","    index = index + 1"]},{"cell_type":"markdown","metadata":{"id":"VegC3a0dan7G"},"source":["Data and Labeling"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1686395118602,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"UDVBZYuq6XrL","outputId":"7daca21c-0188-41ed-844c-ad9080b830bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["(130, 80, 80)\n","float64\n"]}],"source":["\n","Data = np.concatenate((pos_samples, neg_samples), axis =0)\n","#print(data.shape)\n","Y_pos = np.ones(No_pos)\n","Y_neg = np.zeros(No_neg)\n","Label = np.concatenate((Y_pos, Y_neg), axis =0)\n","#train_data, test_data,train_label, test_label = train_test_split(Data, Label, test_size=0.2, shuffle=True )\n","print(Data.shape)\n","# print(Label.shape)\n","Data = Data.reshape(Data.shape[0], Data.shape[1], Data.shape[2], 1)\n","print(Label.dtype)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":705,"status":"ok","timestamp":1686395122471,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"UtWbdf6U3X4g","outputId":"bd8d99c9-3e3c-4b68-9e91-e184b0d429ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["(130, 80, 80, 1)\n","(126, 80, 80, 1)\n","(130,)\n"]}],"source":["print(Data.shape)\n","Data = np.delete(Data,[94, 95, 104, 108],axis=0)\n","print(Data.shape)\n","print(Label.shape)\n","# Label = np.delete(Label,delete_index)\n","# print(Label.shape)\n","# print(Label)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686395137316,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"shV0wf1FfHB7","outputId":"a900d3d4-94de-4aa7-d909-5cdf4e5045cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["(126, 80, 80, 1)\n","(122, 80, 80, 1)\n","(130,)\n"]}],"source":["print(Data.shape)\n","Data = np.delete(Data,[109,110,119, 125],axis=0)\n","print(Data.shape)\n","print(Label.shape)"]},{"cell_type":"markdown","metadata":{"id":"i3Wsfdce83xd"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","Deep Learning Model"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":640,"status":"ok","timestamp":1686395141327,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"cEoAuX2x87is"},"outputs":[],"source":["def create_model():\n","  model = Sequential()\n","  model.add(Conv2D(filters = 32, kernel_size = 3, padding='valid',activation = 'relu', strides=(1, 1),input_shape = (80,80,1)))\n","  model.add(AveragePooling2D(pool_size=(2, 2)))\n","  model.add(Conv2D(filters = 64, kernel_size = 3, strides=(1, 1),padding='valid', activation = 'relu'))\n","  model.add(AveragePooling2D(pool_size=(2, 2)))\n","  model.add(Conv2D(filters = 128, kernel_size = 3, strides=(1, 1),padding='valid', activation = 'relu'))\n","  model.add(Flatten())\n","  model.add(Dense(256, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(64, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(2, activation='softmax'))\n","  model.summary()\n","  opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n","  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","  return model"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1523,"status":"ok","timestamp":1686395149774,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"8uxGCuwfY1Ix","outputId":"080bdc39-900a-41af-b1c9-71bd878f05d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 78, 78, 32)        320       \n","                                                                 \n"," average_pooling2d (AverageP  (None, 39, 39, 32)       0         \n"," ooling2D)                                                       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 37, 37, 64)        18496     \n","                                                                 \n"," average_pooling2d_1 (Averag  (None, 18, 18, 64)       0         \n"," ePooling2D)                                                     \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n","                                                                 \n"," flatten (Flatten)           (None, 32768)             0         \n","                                                                 \n"," dense (Dense)               (None, 256)               8388864   \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                16448     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 8,498,114\n","Trainable params: 8,498,114\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model = Sequential()\n","model.add(Conv2D(filters = 32, kernel_size = 3, padding='valid',activation = 'relu', strides=(1, 1),input_shape = (80,80,1)))\n","model.add(AveragePooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(filters = 64, kernel_size = 3, strides=(1, 1),padding='valid', activation = 'relu'))\n","model.add(AveragePooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(filters = 128, kernel_size = 3, strides=(1, 1),padding='valid', activation = 'relu'))\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(2, activation='softmax'))\n","model.summary()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1686395160499,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"},"user_tz":-330},"id":"jY4OF-D0gCkF"},"outputs":[],"source":["def train_and_evaluate__model(model, data_train, labels_train, data_test, labels_test):\n","  # callback = [TerminateOnBaseline(monitor='accuracy', baseline=1)]\n","  # callback = myCallback()\n","  print(\"Train Labels\",labels_train)\n","  model.fit(x=data_train, y=labels_train, batch_size=64, epochs=100)\n","  prediticted_classes = model.predict(data_test, batch_size=4)\n","  print(\"Predict Classes\", prediticted_classes)\n","  print(\"Evaluate on test data\")\n","  results = model.evaluate(data_test, labels_test, verbose=0)\n","  print(\"test loss, test acc:\", results)\n","  #test_labels = test_graphs.values\n","  #print(labels_test)\n","  #print('\\nClassification Report\\n')\n","  #print(classification_report(labels_test, prediticted_classes,  target_names=['Class 0', 'Class 1']))\n","  #print(confusion_matrix(labels_test, prediticted_classes))\n","  return results[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e68cpbSHdkhb"},"outputs":[],"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"I7f_ZzJvhLjT"},"source":["K-Fold"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffCSSOrshOg-","executionInfo":{"status":"ok","timestamp":1686402356716,"user_tz":-330,"elapsed":845696,"user":{"displayName":"V Subaramya","userId":"05082926859515857803"}},"outputId":"f531e265-6345-4b12-ac55-95875e468a9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  16  17  18  19\n","  20  21  22  25  26  27  28  30  31  32  33  34  35  36  38  40  41  42\n","  43  44  45  46  47  48  49  51  52  53  54  55  56  58  59  60  61  62\n","  63  64  65  66  68  69  70  75  76  78  79  80  82  83  84  85  86  87\n","  88  89  90  92  94  95  96  97  98  99 100 101 102 103 104 105 106 109\n"," 110 111 116 117 119 120 121] TEST: [ 14  15  23  24  29  37  39  50  57  67  71  72  73  74  77  81  91  93\n"," 107 108 112 113 114 115 118]\n","Test label [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0.]\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 78, 78, 32)        320       \n","                                                                 \n"," average_pooling2d (AverageP  (None, 39, 39, 32)       0         \n"," ooling2D)                                                       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 37, 37, 64)        18496     \n","                                                                 \n"," average_pooling2d_1 (Averag  (None, 18, 18, 64)       0         \n"," ePooling2D)                                                     \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n","                                                                 \n"," flatten (Flatten)           (None, 32768)             0         \n","                                                                 \n"," dense (Dense)               (None, 256)               8388864   \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                16448     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 8,498,114\n","Trainable params: 8,498,114\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train Labels [[0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]]\n","Epoch 1/100\n","2/2 [==============================] - 3s 445ms/step - loss: 0.6905 - accuracy: 0.5464\n","Epoch 2/100\n","2/2 [==============================] - 1s 427ms/step - loss: 0.6788 - accuracy: 0.5567\n","Epoch 3/100\n","2/2 [==============================] - 1s 445ms/step - loss: 0.6810 - accuracy: 0.6289\n","Epoch 4/100\n","2/2 [==============================] - 2s 832ms/step - loss: 0.6679 - accuracy: 0.6082\n","Epoch 5/100\n","2/2 [==============================] - 2s 784ms/step - loss: 0.6285 - accuracy: 0.6907\n","Epoch 6/100\n","2/2 [==============================] - 2s 441ms/step - loss: 0.6319 - accuracy: 0.6907\n","Epoch 7/100\n","2/2 [==============================] - 1s 448ms/step - loss: 0.5664 - accuracy: 0.8247\n","Epoch 8/100\n","2/2 [==============================] - 1s 421ms/step - loss: 0.4849 - accuracy: 0.8247\n","Epoch 9/100\n","2/2 [==============================] - 1s 425ms/step - loss: 0.4450 - accuracy: 0.8557\n","Epoch 10/100\n","2/2 [==============================] - 1s 406ms/step - loss: 0.3961 - accuracy: 0.8144\n","Epoch 11/100\n","2/2 [==============================] - 1s 416ms/step - loss: 0.4736 - accuracy: 0.7835\n","Epoch 12/100\n","2/2 [==============================] - 1s 407ms/step - loss: 0.3312 - accuracy: 0.8660\n","Epoch 13/100\n","2/2 [==============================] - 1s 435ms/step - loss: 0.3247 - accuracy: 0.8763\n","Epoch 14/100\n","2/2 [==============================] - 1s 417ms/step - loss: 0.2797 - accuracy: 0.9072\n","Epoch 15/100\n","2/2 [==============================] - 2s 798ms/step - loss: 0.2503 - accuracy: 0.9072\n","Epoch 16/100\n","2/2 [==============================] - 2s 804ms/step - loss: 0.3078 - accuracy: 0.8763\n","Epoch 17/100\n","2/2 [==============================] - 1s 402ms/step - loss: 0.2409 - accuracy: 0.9278\n","Epoch 18/100\n","2/2 [==============================] - 1s 398ms/step - loss: 0.2233 - accuracy: 0.9278\n","Epoch 19/100\n","2/2 [==============================] - 1s 399ms/step - loss: 0.2079 - accuracy: 0.9381\n","Epoch 20/100\n","2/2 [==============================] - 1s 427ms/step - loss: 0.2079 - accuracy: 0.9175\n","Epoch 21/100\n","2/2 [==============================] - 1s 424ms/step - loss: 0.1529 - accuracy: 0.9381\n","Epoch 22/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.1780 - accuracy: 0.9485\n","Epoch 23/100\n","2/2 [==============================] - 1s 427ms/step - loss: 0.1379 - accuracy: 0.9485\n","Epoch 24/100\n","2/2 [==============================] - 1s 433ms/step - loss: 0.2244 - accuracy: 0.9072\n","Epoch 25/100\n","2/2 [==============================] - 1s 690ms/step - loss: 0.1151 - accuracy: 0.9691\n","Epoch 26/100\n","2/2 [==============================] - 2s 784ms/step - loss: 0.1533 - accuracy: 0.9381\n","Epoch 27/100\n","2/2 [==============================] - 2s 713ms/step - loss: 0.1304 - accuracy: 0.9588\n","Epoch 28/100\n","2/2 [==============================] - 1s 434ms/step - loss: 0.1264 - accuracy: 0.9485\n","Epoch 29/100\n","2/2 [==============================] - 1s 417ms/step - loss: 0.1834 - accuracy: 0.9278\n","Epoch 30/100\n","2/2 [==============================] - 1s 446ms/step - loss: 0.1124 - accuracy: 0.9794\n","Epoch 31/100\n","2/2 [==============================] - 1s 411ms/step - loss: 0.1314 - accuracy: 0.9588\n","Epoch 32/100\n","2/2 [==============================] - 1s 407ms/step - loss: 0.0548 - accuracy: 0.9897\n","Epoch 33/100\n","2/2 [==============================] - 1s 424ms/step - loss: 0.1356 - accuracy: 0.9588\n","Epoch 34/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0905 - accuracy: 0.9588\n","Epoch 35/100\n","2/2 [==============================] - 1s 441ms/step - loss: 0.0714 - accuracy: 0.9794\n","Epoch 36/100\n","2/2 [==============================] - 2s 774ms/step - loss: 0.0695 - accuracy: 0.9794\n","Epoch 37/100\n","2/2 [==============================] - 2s 807ms/step - loss: 0.0347 - accuracy: 0.9897\n","Epoch 38/100\n","2/2 [==============================] - 2s 630ms/step - loss: 0.0289 - accuracy: 0.9897\n","Epoch 39/100\n","2/2 [==============================] - 1s 447ms/step - loss: 0.0423 - accuracy: 0.9794\n","Epoch 40/100\n","2/2 [==============================] - 1s 430ms/step - loss: 0.0251 - accuracy: 1.0000\n","Epoch 41/100\n","2/2 [==============================] - 1s 419ms/step - loss: 0.1022 - accuracy: 0.9691\n","Epoch 42/100\n","2/2 [==============================] - 1s 431ms/step - loss: 0.0252 - accuracy: 1.0000\n","Epoch 43/100\n","2/2 [==============================] - 1s 426ms/step - loss: 0.0471 - accuracy: 0.9794\n","Epoch 44/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0115 - accuracy: 1.0000\n","Epoch 45/100\n","2/2 [==============================] - 1s 453ms/step - loss: 0.0218 - accuracy: 1.0000\n","Epoch 46/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0288 - accuracy: 1.0000\n","Epoch 47/100\n","2/2 [==============================] - 2s 788ms/step - loss: 0.0353 - accuracy: 0.9897\n","Epoch 48/100\n","2/2 [==============================] - 2s 760ms/step - loss: 0.0501 - accuracy: 0.9794\n","Epoch 49/100\n","2/2 [==============================] - 2s 466ms/step - loss: 0.0236 - accuracy: 1.0000\n","Epoch 50/100\n","2/2 [==============================] - 1s 428ms/step - loss: 0.0154 - accuracy: 1.0000\n","Epoch 51/100\n","2/2 [==============================] - 1s 453ms/step - loss: 0.0240 - accuracy: 0.9897\n","Epoch 52/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.0148 - accuracy: 1.0000\n","Epoch 53/100\n","2/2 [==============================] - 1s 466ms/step - loss: 0.0116 - accuracy: 1.0000\n","Epoch 54/100\n","2/2 [==============================] - 1s 435ms/step - loss: 0.0149 - accuracy: 1.0000\n","Epoch 55/100\n","2/2 [==============================] - 1s 438ms/step - loss: 0.0051 - accuracy: 1.0000\n","Epoch 56/100\n","2/2 [==============================] - 1s 434ms/step - loss: 0.0072 - accuracy: 1.0000\n","Epoch 57/100\n","2/2 [==============================] - 1s 440ms/step - loss: 0.0056 - accuracy: 1.0000\n","Epoch 58/100\n","2/2 [==============================] - 2s 797ms/step - loss: 0.0027 - accuracy: 1.0000\n","Epoch 59/100\n","2/2 [==============================] - 2s 791ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 60/100\n","2/2 [==============================] - 1s 411ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 61/100\n","2/2 [==============================] - 1s 434ms/step - loss: 0.0018 - accuracy: 1.0000\n","Epoch 62/100\n","2/2 [==============================] - 1s 412ms/step - loss: 0.0023 - accuracy: 1.0000\n","Epoch 63/100\n","2/2 [==============================] - 1s 438ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 64/100\n","2/2 [==============================] - 1s 421ms/step - loss: 0.0035 - accuracy: 1.0000\n","Epoch 65/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 66/100\n","2/2 [==============================] - 1s 411ms/step - loss: 0.0036 - accuracy: 1.0000\n","Epoch 67/100\n","2/2 [==============================] - 1s 414ms/step - loss: 7.2248e-04 - accuracy: 1.0000\n","Epoch 68/100\n","2/2 [==============================] - 1s 590ms/step - loss: 0.0011 - accuracy: 1.0000\n","Epoch 69/100\n","2/2 [==============================] - 2s 733ms/step - loss: 0.0028 - accuracy: 1.0000\n","Epoch 70/100\n","2/2 [==============================] - 2s 784ms/step - loss: 0.0079 - accuracy: 1.0000\n","Epoch 71/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 72/100\n","2/2 [==============================] - 1s 419ms/step - loss: 7.0284e-04 - accuracy: 1.0000\n","Epoch 73/100\n","2/2 [==============================] - 1s 443ms/step - loss: 0.0018 - accuracy: 1.0000\n","Epoch 74/100\n","2/2 [==============================] - 1s 434ms/step - loss: 0.0021 - accuracy: 1.0000\n","Epoch 75/100\n","2/2 [==============================] - 1s 415ms/step - loss: 0.0074 - accuracy: 1.0000\n","Epoch 76/100\n","2/2 [==============================] - 1s 436ms/step - loss: 0.0033 - accuracy: 1.0000\n","Epoch 77/100\n","2/2 [==============================] - 1s 446ms/step - loss: 0.0098 - accuracy: 1.0000\n","Epoch 78/100\n","2/2 [==============================] - 1s 453ms/step - loss: 0.0044 - accuracy: 1.0000\n","Epoch 79/100\n","2/2 [==============================] - 2s 787ms/step - loss: 0.0159 - accuracy: 0.9897\n","Epoch 80/100\n","2/2 [==============================] - 2s 810ms/step - loss: 0.0037 - accuracy: 1.0000\n","Epoch 81/100\n","2/2 [==============================] - 2s 475ms/step - loss: 0.0044 - accuracy: 1.0000\n","Epoch 82/100\n","2/2 [==============================] - 1s 429ms/step - loss: 0.0062 - accuracy: 1.0000\n","Epoch 83/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0073 - accuracy: 1.0000\n","Epoch 84/100\n","2/2 [==============================] - 1s 416ms/step - loss: 0.0035 - accuracy: 1.0000\n","Epoch 85/100\n","2/2 [==============================] - 1s 430ms/step - loss: 0.0055 - accuracy: 1.0000\n","Epoch 86/100\n","2/2 [==============================] - 1s 399ms/step - loss: 0.0054 - accuracy: 1.0000\n","Epoch 87/100\n","2/2 [==============================] - 1s 415ms/step - loss: 0.0034 - accuracy: 1.0000\n","Epoch 88/100\n","2/2 [==============================] - 1s 433ms/step - loss: 8.4554e-04 - accuracy: 1.0000\n","Epoch 89/100\n","2/2 [==============================] - 1s 456ms/step - loss: 0.0024 - accuracy: 1.0000\n","Epoch 90/100\n","2/2 [==============================] - 2s 795ms/step - loss: 0.0027 - accuracy: 1.0000\n","Epoch 91/100\n","2/2 [==============================] - 2s 740ms/step - loss: 0.0049 - accuracy: 1.0000\n","Epoch 92/100\n","2/2 [==============================] - 2s 422ms/step - loss: 0.0012 - accuracy: 1.0000\n","Epoch 93/100\n","2/2 [==============================] - 1s 411ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 94/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.0047 - accuracy: 1.0000\n","Epoch 95/100\n","2/2 [==============================] - 1s 429ms/step - loss: 6.4984e-04 - accuracy: 1.0000\n","Epoch 96/100\n","2/2 [==============================] - 1s 425ms/step - loss: 0.0019 - accuracy: 1.0000\n","Epoch 97/100\n","2/2 [==============================] - 1s 440ms/step - loss: 4.3032e-04 - accuracy: 1.0000\n","Epoch 98/100\n","2/2 [==============================] - 1s 438ms/step - loss: 4.4610e-04 - accuracy: 1.0000\n","Epoch 99/100\n","2/2 [==============================] - 1s 402ms/step - loss: 1.5211e-04 - accuracy: 1.0000\n","Epoch 100/100\n","2/2 [==============================] - 1s 422ms/step - loss: 4.4978e-04 - accuracy: 1.0000\n","7/7 [==============================] - 0s 22ms/step\n","Predict Classes [[7.88030775e-06 9.99992132e-01]\n"," [1.91484764e-06 9.99998093e-01]\n"," [2.27578287e-03 9.97724235e-01]\n"," [1.96148910e-08 1.00000000e+00]\n"," [1.59523588e-05 9.99984026e-01]\n"," [1.78903283e-05 9.99982119e-01]\n"," [1.61398275e-05 9.99983907e-01]\n"," [1.23764300e-06 9.99998808e-01]\n"," [1.61584826e-08 1.00000000e+00]\n"," [9.99282300e-01 7.17699644e-04]\n"," [1.00000000e+00 8.71347350e-09]\n"," [9.99999642e-01 3.78339422e-07]\n"," [1.00000000e+00 7.68724806e-10]\n"," [9.99999881e-01 1.27694776e-07]\n"," [9.99999881e-01 1.39749687e-07]\n"," [9.99999881e-01 1.11308125e-07]\n"," [1.00000000e+00 7.76979172e-15]\n"," [1.00000000e+00 1.55079878e-08]\n"," [9.48514998e-01 5.14849983e-02]\n"," [2.14801971e-02 9.78519857e-01]\n"," [1.00000000e+00 2.62842459e-08]\n"," [9.99999642e-01 3.72997079e-07]\n"," [9.99984145e-01 1.58163421e-05]\n"," [9.99999881e-01 1.13023134e-07]\n"," [1.00000000e+00 4.53556290e-12]]\n","Evaluate on test data\n","test loss, test acc: [0.15586219727993011, 0.9599999785423279]\n","7/7 [==============================] - 0s 20ms/step\n","\n","Classification Report\n","\n","[1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0.]\n","(25,)\n","[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n","(25,)\n","              precision    recall  f1-score   support\n","\n","     Class 0       1.00      0.94      0.97        16\n","     Class 1       0.90      1.00      0.95         9\n","\n","    accuracy                           0.96        25\n","   macro avg       0.95      0.97      0.96        25\n","weighted avg       0.96      0.96      0.96        25\n","\n","[[15  1]\n"," [ 0  9]]\n","TRAIN: [  1   2   4   6   7   9  11  12  13  14  15  16  17  19  20  22  23  24\n","  25  26  27  28  29  30  31  32  33  34  35  36  37  39  40  43  44  45\n","  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  64  67\n","  68  70  71  72  73  74  75  76  77  79  80  81  83  85  86  87  88  89\n","  91  92  93  95  97  99 100 101 102 104 105 106 107 108 109 110 111 112\n"," 113 114 115 116 117 118 121] TEST: [  0   3   5   8  10  18  21  38  41  42  62  63  65  66  69  78  82  84\n","  90  94  96  98 103 119 120]\n","Test label [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0.]\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 78, 78, 32)        320       \n","                                                                 \n"," average_pooling2d (AverageP  (None, 39, 39, 32)       0         \n"," ooling2D)                                                       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 37, 37, 64)        18496     \n","                                                                 \n"," average_pooling2d_1 (Averag  (None, 18, 18, 64)       0         \n"," ePooling2D)                                                     \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n","                                                                 \n"," flatten (Flatten)           (None, 32768)             0         \n","                                                                 \n"," dense (Dense)               (None, 256)               8388864   \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                16448     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 8,498,114\n","Trainable params: 8,498,114\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train Labels [[0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]]\n","Epoch 1/100\n","2/2 [==============================] - 2s 416ms/step - loss: 0.6930 - accuracy: 0.5258\n","Epoch 2/100\n","2/2 [==============================] - 1s 401ms/step - loss: 0.6836 - accuracy: 0.6392\n","Epoch 3/100\n","2/2 [==============================] - 1s 424ms/step - loss: 0.6420 - accuracy: 0.6701\n","Epoch 4/100\n","2/2 [==============================] - 2s 755ms/step - loss: 0.5966 - accuracy: 0.7216\n","Epoch 5/100\n","2/2 [==============================] - 2s 848ms/step - loss: 0.4819 - accuracy: 0.7732\n","Epoch 6/100\n","2/2 [==============================] - 2s 777ms/step - loss: 0.5277 - accuracy: 0.7216\n","Epoch 7/100\n","2/2 [==============================] - 2s 792ms/step - loss: 0.4810 - accuracy: 0.7629\n","Epoch 8/100\n","2/2 [==============================] - 2s 461ms/step - loss: 0.3960 - accuracy: 0.8144\n","Epoch 9/100\n","2/2 [==============================] - 1s 408ms/step - loss: 0.3060 - accuracy: 0.9175\n","Epoch 10/100\n","2/2 [==============================] - 1s 425ms/step - loss: 0.2200 - accuracy: 0.9588\n","Epoch 11/100\n","2/2 [==============================] - 1s 402ms/step - loss: 0.2083 - accuracy: 0.9588\n","Epoch 12/100\n","2/2 [==============================] - 1s 420ms/step - loss: 0.2406 - accuracy: 0.9278\n","Epoch 13/100\n","2/2 [==============================] - 1s 399ms/step - loss: 0.2826 - accuracy: 0.8763\n","Epoch 14/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.2491 - accuracy: 0.9072\n","Epoch 15/100\n","2/2 [==============================] - 1s 436ms/step - loss: 0.2033 - accuracy: 0.9381\n","Epoch 16/100\n","2/2 [==============================] - 1s 403ms/step - loss: 0.1408 - accuracy: 0.9381\n","Epoch 17/100\n","2/2 [==============================] - 2s 804ms/step - loss: 0.1646 - accuracy: 0.9381\n","Epoch 18/100\n","2/2 [==============================] - 2s 793ms/step - loss: 0.1621 - accuracy: 0.9278\n","Epoch 19/100\n","2/2 [==============================] - 2s 422ms/step - loss: 0.1288 - accuracy: 0.9794\n","Epoch 20/100\n","2/2 [==============================] - 1s 407ms/step - loss: 0.1491 - accuracy: 0.9381\n","Epoch 21/100\n","2/2 [==============================] - 1s 441ms/step - loss: 0.1555 - accuracy: 0.9485\n","Epoch 22/100\n","2/2 [==============================] - 1s 409ms/step - loss: 0.1184 - accuracy: 0.9588\n","Epoch 23/100\n","2/2 [==============================] - 1s 410ms/step - loss: 0.0854 - accuracy: 0.9897\n","Epoch 24/100\n","2/2 [==============================] - 1s 397ms/step - loss: 0.0970 - accuracy: 0.9691\n","Epoch 25/100\n","2/2 [==============================] - 1s 426ms/step - loss: 0.0978 - accuracy: 0.9588\n","Epoch 26/100\n","2/2 [==============================] - 1s 456ms/step - loss: 0.1728 - accuracy: 0.9381\n","Epoch 27/100\n","2/2 [==============================] - 1s 425ms/step - loss: 0.1290 - accuracy: 0.9588\n","Epoch 28/100\n","2/2 [==============================] - 2s 806ms/step - loss: 0.0908 - accuracy: 0.9588\n","Epoch 29/100\n","2/2 [==============================] - 2s 783ms/step - loss: 0.1097 - accuracy: 0.9691\n","Epoch 30/100\n","2/2 [==============================] - 1s 414ms/step - loss: 0.0722 - accuracy: 0.9588\n","Epoch 31/100\n","2/2 [==============================] - 1s 400ms/step - loss: 0.0720 - accuracy: 0.9794\n","Epoch 32/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.0883 - accuracy: 0.9588\n","Epoch 33/100\n","2/2 [==============================] - 1s 429ms/step - loss: 0.0670 - accuracy: 0.9794\n","Epoch 34/100\n","2/2 [==============================] - 1s 399ms/step - loss: 0.0606 - accuracy: 0.9794\n","Epoch 35/100\n","2/2 [==============================] - 1s 400ms/step - loss: 0.0422 - accuracy: 0.9897\n","Epoch 36/100\n","2/2 [==============================] - 1s 432ms/step - loss: 0.0369 - accuracy: 0.9897\n","Epoch 37/100\n","2/2 [==============================] - 1s 437ms/step - loss: 0.1008 - accuracy: 0.9485\n","Epoch 38/100\n","2/2 [==============================] - 1s 417ms/step - loss: 0.0463 - accuracy: 0.9897\n","Epoch 39/100\n","2/2 [==============================] - 2s 786ms/step - loss: 0.0615 - accuracy: 0.9794\n","Epoch 40/100\n","2/2 [==============================] - 2s 807ms/step - loss: 0.0332 - accuracy: 0.9897\n","Epoch 41/100\n","2/2 [==============================] - 1s 408ms/step - loss: 0.0619 - accuracy: 0.9794\n","Epoch 42/100\n","2/2 [==============================] - 1s 423ms/step - loss: 0.0350 - accuracy: 0.9897\n","Epoch 43/100\n","2/2 [==============================] - 1s 408ms/step - loss: 0.0293 - accuracy: 0.9897\n","Epoch 44/100\n","2/2 [==============================] - 1s 445ms/step - loss: 0.0138 - accuracy: 1.0000\n","Epoch 45/100\n","2/2 [==============================] - 1s 443ms/step - loss: 0.0229 - accuracy: 1.0000\n","Epoch 46/100\n","2/2 [==============================] - 1s 396ms/step - loss: 0.0210 - accuracy: 1.0000\n","Epoch 47/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.0597 - accuracy: 0.9897\n","Epoch 48/100\n","2/2 [==============================] - 1s 433ms/step - loss: 0.0249 - accuracy: 1.0000\n","Epoch 49/100\n","2/2 [==============================] - 1s 610ms/step - loss: 0.0303 - accuracy: 0.9897\n","Epoch 50/100\n","2/2 [==============================] - 2s 801ms/step - loss: 0.0093 - accuracy: 1.0000\n","Epoch 51/100\n","2/2 [==============================] - 2s 785ms/step - loss: 0.0175 - accuracy: 0.9897\n","Epoch 52/100\n","2/2 [==============================] - 1s 448ms/step - loss: 0.0159 - accuracy: 0.9897\n","Epoch 53/100\n","2/2 [==============================] - 1s 424ms/step - loss: 0.0125 - accuracy: 1.0000\n","Epoch 54/100\n","2/2 [==============================] - 1s 397ms/step - loss: 0.0161 - accuracy: 0.9897\n","Epoch 55/100\n","2/2 [==============================] - 1s 415ms/step - loss: 0.0026 - accuracy: 1.0000\n","Epoch 56/100\n","2/2 [==============================] - 1s 406ms/step - loss: 0.0485 - accuracy: 0.9897\n","Epoch 57/100\n","2/2 [==============================] - 1s 409ms/step - loss: 0.0054 - accuracy: 1.0000\n","Epoch 58/100\n","2/2 [==============================] - 1s 443ms/step - loss: 0.0206 - accuracy: 1.0000\n","Epoch 59/100\n","2/2 [==============================] - 1s 446ms/step - loss: 0.0081 - accuracy: 1.0000\n","Epoch 60/100\n","2/2 [==============================] - 1s 724ms/step - loss: 0.0037 - accuracy: 1.0000\n","Epoch 61/100\n","2/2 [==============================] - 2s 774ms/step - loss: 0.0105 - accuracy: 1.0000\n","Epoch 62/100\n","2/2 [==============================] - 2s 683ms/step - loss: 0.0194 - accuracy: 1.0000\n","Epoch 63/100\n","2/2 [==============================] - 1s 436ms/step - loss: 0.0031 - accuracy: 1.0000\n","Epoch 64/100\n","2/2 [==============================] - 1s 431ms/step - loss: 0.0069 - accuracy: 1.0000\n","Epoch 65/100\n","2/2 [==============================] - 1s 441ms/step - loss: 0.0034 - accuracy: 1.0000\n","Epoch 66/100\n","2/2 [==============================] - 1s 436ms/step - loss: 0.0062 - accuracy: 1.0000\n","Epoch 67/100\n","2/2 [==============================] - 1s 415ms/step - loss: 0.0044 - accuracy: 1.0000\n","Epoch 68/100\n","2/2 [==============================] - 1s 416ms/step - loss: 0.0040 - accuracy: 1.0000\n","Epoch 69/100\n","2/2 [==============================] - 1s 418ms/step - loss: 0.0021 - accuracy: 1.0000\n","Epoch 70/100\n","2/2 [==============================] - 1s 450ms/step - loss: 0.0083 - accuracy: 1.0000\n","Epoch 71/100\n","2/2 [==============================] - 2s 731ms/step - loss: 0.0036 - accuracy: 1.0000\n","Epoch 72/100\n","2/2 [==============================] - 2s 772ms/step - loss: 0.0018 - accuracy: 1.0000\n","Epoch 73/100\n","2/2 [==============================] - 2s 414ms/step - loss: 0.0143 - accuracy: 1.0000\n","Epoch 74/100\n","2/2 [==============================] - 1s 441ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 75/100\n","2/2 [==============================] - 1s 401ms/step - loss: 0.0162 - accuracy: 0.9897\n","Epoch 76/100\n","2/2 [==============================] - 1s 415ms/step - loss: 8.6416e-04 - accuracy: 1.0000\n","Epoch 77/100\n","2/2 [==============================] - 1s 434ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 78/100\n","2/2 [==============================] - 1s 441ms/step - loss: 0.0100 - accuracy: 0.9897\n","Epoch 79/100\n","2/2 [==============================] - 1s 448ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 80/100\n","2/2 [==============================] - 1s 429ms/step - loss: 0.0164 - accuracy: 0.9897\n","Epoch 81/100\n","2/2 [==============================] - 1s 407ms/step - loss: 0.0047 - accuracy: 1.0000\n","Epoch 82/100\n","2/2 [==============================] - 2s 802ms/step - loss: 0.0022 - accuracy: 1.0000\n","Epoch 83/100\n","2/2 [==============================] - 2s 801ms/step - loss: 0.0025 - accuracy: 1.0000\n","Epoch 84/100\n","2/2 [==============================] - 1s 421ms/step - loss: 0.0104 - accuracy: 1.0000\n","Epoch 85/100\n","2/2 [==============================] - 1s 442ms/step - loss: 0.0102 - accuracy: 1.0000\n","Epoch 86/100\n","2/2 [==============================] - 1s 448ms/step - loss: 0.0023 - accuracy: 1.0000\n","Epoch 87/100\n","2/2 [==============================] - 1s 432ms/step - loss: 0.0073 - accuracy: 1.0000\n","Epoch 88/100\n","2/2 [==============================] - 1s 417ms/step - loss: 0.0047 - accuracy: 1.0000\n","Epoch 89/100\n","2/2 [==============================] - 1s 431ms/step - loss: 0.0020 - accuracy: 1.0000\n","Epoch 90/100\n","2/2 [==============================] - 1s 406ms/step - loss: 0.0030 - accuracy: 1.0000\n","Epoch 91/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.0023 - accuracy: 1.0000\n","Epoch 92/100\n","2/2 [==============================] - 1s 615ms/step - loss: 0.0011 - accuracy: 1.0000\n","Epoch 93/100\n","2/2 [==============================] - 2s 742ms/step - loss: 0.0034 - accuracy: 1.0000\n","Epoch 94/100\n","2/2 [==============================] - 2s 693ms/step - loss: 0.0077 - accuracy: 1.0000\n","Epoch 95/100\n","2/2 [==============================] - 1s 412ms/step - loss: 0.0043 - accuracy: 1.0000\n","Epoch 96/100\n","2/2 [==============================] - 1s 432ms/step - loss: 0.0023 - accuracy: 1.0000\n","Epoch 97/100\n","2/2 [==============================] - 1s 415ms/step - loss: 0.0028 - accuracy: 1.0000\n","Epoch 98/100\n","2/2 [==============================] - 1s 448ms/step - loss: 0.0013 - accuracy: 1.0000\n","Epoch 99/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.0024 - accuracy: 1.0000\n","Epoch 100/100\n","2/2 [==============================] - 1s 413ms/step - loss: 0.0015 - accuracy: 1.0000\n","7/7 [==============================] - 0s 20ms/step\n","Predict Classes [[1.86979810e-06 9.99998093e-01]\n"," [1.32418994e-04 9.99867558e-01]\n"," [3.43725733e-05 9.99965668e-01]\n"," [2.62156368e-06 9.99997377e-01]\n"," [1.01657846e-04 9.99898314e-01]\n"," [9.05862689e-01 9.41372961e-02]\n"," [1.23469380e-03 9.98765349e-01]\n"," [3.62663485e-07 9.99999642e-01]\n"," [3.64843866e-09 1.00000000e+00]\n"," [2.56087219e-06 9.99997497e-01]\n"," [1.00000000e+00 5.18806664e-10]\n"," [1.00000000e+00 7.04103220e-09]\n"," [9.99999881e-01 1.06433824e-07]\n"," [8.32120240e-01 1.67879760e-01]\n"," [9.99969244e-01 3.07503105e-05]\n"," [1.00000000e+00 1.43740629e-12]\n"," [1.00000000e+00 5.01414021e-09]\n"," [9.84735012e-01 1.52649991e-02]\n"," [1.00000000e+00 4.31617320e-09]\n"," [1.44169515e-03 9.98558342e-01]\n"," [9.99999881e-01 1.12734739e-07]\n"," [1.00000000e+00 7.25222630e-14]\n"," [1.00000000e+00 9.06059117e-10]\n"," [9.99984026e-01 1.59642750e-05]\n"," [1.00000000e+00 6.57579102e-10]]\n","Evaluate on test data\n","test loss, test acc: [0.36422625184059143, 0.9200000166893005]\n","7/7 [==============================] - 0s 20ms/step\n","\n","Classification Report\n","\n","[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0.]\n","(25,)\n","[1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n","(25,)\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.93      0.93      0.93        15\n","     Class 1       0.90      0.90      0.90        10\n","\n","    accuracy                           0.92        25\n","   macro avg       0.92      0.92      0.92        25\n","weighted avg       0.92      0.92      0.92        25\n","\n","[[14  1]\n"," [ 1  9]]\n","TRAIN: [  0   1   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19  21\n","  22  23  24  26  28  29  30  31  33  34  36  37  38  39  40  41  42  43\n","  44  46  48  49  50  53  54  56  57  60  61  62  63  65  66  67  68  69\n","  70  71  72  73  74  75  76  77  78  79  80  81  82  84  85  87  89  90\n","  91  93  94  95  96  98  99 100 102 103 104 105 106 107 108 109 110 112\n"," 113 114 115 117 118 119 120 121] TEST: [  2  11  12  20  25  27  32  35  45  47  51  52  55  58  59  64  83  86\n","  88  92  97 101 111 116]\n","Test label [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 78, 78, 32)        320       \n","                                                                 \n"," average_pooling2d (AverageP  (None, 39, 39, 32)       0         \n"," ooling2D)                                                       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 37, 37, 64)        18496     \n","                                                                 \n"," average_pooling2d_1 (Averag  (None, 18, 18, 64)       0         \n"," ePooling2D)                                                     \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n","                                                                 \n"," flatten (Flatten)           (None, 32768)             0         \n","                                                                 \n"," dense (Dense)               (None, 256)               8388864   \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                16448     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 8,498,114\n","Trainable params: 8,498,114\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train Labels [[0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]]\n","Epoch 1/100\n","2/2 [==============================] - 4s 772ms/step - loss: 0.6908 - accuracy: 0.5000\n","Epoch 2/100\n","2/2 [==============================] - 1s 429ms/step - loss: 0.6912 - accuracy: 0.5204\n","Epoch 3/100\n","2/2 [==============================] - 1s 463ms/step - loss: 0.6574 - accuracy: 0.5510\n","Epoch 4/100\n","2/2 [==============================] - 1s 441ms/step - loss: 0.6125 - accuracy: 0.6837\n","Epoch 5/100\n","2/2 [==============================] - 1s 416ms/step - loss: 0.5869 - accuracy: 0.6531\n","Epoch 6/100\n","2/2 [==============================] - 1s 416ms/step - loss: 0.5470 - accuracy: 0.7449\n","Epoch 7/100\n","2/2 [==============================] - 1s 414ms/step - loss: 0.5922 - accuracy: 0.6531\n","Epoch 8/100\n","2/2 [==============================] - 1s 455ms/step - loss: 0.4860 - accuracy: 0.7857\n","Epoch 9/100\n","2/2 [==============================] - 1s 451ms/step - loss: 0.4231 - accuracy: 0.8571\n","Epoch 10/100\n","2/2 [==============================] - 2s 814ms/step - loss: 0.3787 - accuracy: 0.8776\n","Epoch 11/100\n","2/2 [==============================] - 2s 837ms/step - loss: 0.3511 - accuracy: 0.8878\n","Epoch 12/100\n","2/2 [==============================] - 2s 655ms/step - loss: 0.2952 - accuracy: 0.8980\n","Epoch 13/100\n","2/2 [==============================] - 1s 428ms/step - loss: 0.2595 - accuracy: 0.9082\n","Epoch 14/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.2549 - accuracy: 0.9184\n","Epoch 15/100\n","2/2 [==============================] - 1s 457ms/step - loss: 0.2010 - accuracy: 0.9184\n","Epoch 16/100\n","2/2 [==============================] - 1s 447ms/step - loss: 0.2813 - accuracy: 0.8980\n","Epoch 17/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.1938 - accuracy: 0.9286\n","Epoch 18/100\n","2/2 [==============================] - 1s 436ms/step - loss: 0.1461 - accuracy: 0.9286\n","Epoch 19/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.1739 - accuracy: 0.9388\n","Epoch 20/100\n","2/2 [==============================] - 1s 426ms/step - loss: 0.1044 - accuracy: 0.9694\n","Epoch 21/100\n","2/2 [==============================] - 2s 865ms/step - loss: 0.1618 - accuracy: 0.9388\n","Epoch 22/100\n","2/2 [==============================] - 2s 847ms/step - loss: 0.1480 - accuracy: 0.9184\n","Epoch 23/100\n","2/2 [==============================] - 2s 443ms/step - loss: 0.1378 - accuracy: 0.9388\n","Epoch 24/100\n","2/2 [==============================] - 1s 448ms/step - loss: 0.1007 - accuracy: 0.9592\n","Epoch 25/100\n","2/2 [==============================] - 1s 472ms/step - loss: 0.1057 - accuracy: 0.9694\n","Epoch 26/100\n","2/2 [==============================] - 1s 458ms/step - loss: 0.0751 - accuracy: 0.9898\n","Epoch 27/100\n","2/2 [==============================] - 1s 426ms/step - loss: 0.0617 - accuracy: 0.9796\n","Epoch 28/100\n","2/2 [==============================] - 1s 457ms/step - loss: 0.0966 - accuracy: 0.9592\n","Epoch 29/100\n","2/2 [==============================] - 1s 463ms/step - loss: 0.0473 - accuracy: 0.9898\n","Epoch 30/100\n","2/2 [==============================] - 1s 437ms/step - loss: 0.0602 - accuracy: 0.9796\n","Epoch 31/100\n","2/2 [==============================] - 2s 848ms/step - loss: 0.0493 - accuracy: 0.9898\n","Epoch 32/100\n","2/2 [==============================] - 2s 852ms/step - loss: 0.0185 - accuracy: 1.0000\n","Epoch 33/100\n","2/2 [==============================] - 2s 463ms/step - loss: 0.0348 - accuracy: 0.9898\n","Epoch 34/100\n","2/2 [==============================] - 1s 459ms/step - loss: 0.0946 - accuracy: 0.9796\n","Epoch 35/100\n","2/2 [==============================] - 1s 443ms/step - loss: 0.1425 - accuracy: 0.9388\n","Epoch 36/100\n","2/2 [==============================] - 1s 459ms/step - loss: 0.1518 - accuracy: 0.9490\n","Epoch 37/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.0542 - accuracy: 0.9796\n","Epoch 38/100\n","2/2 [==============================] - 1s 433ms/step - loss: 0.1188 - accuracy: 0.9388\n","Epoch 39/100\n","2/2 [==============================] - 1s 451ms/step - loss: 0.0593 - accuracy: 0.9796\n","Epoch 40/100\n","2/2 [==============================] - 1s 464ms/step - loss: 0.0722 - accuracy: 0.9796\n","Epoch 41/100\n","2/2 [==============================] - 1s 622ms/step - loss: 0.0241 - accuracy: 1.0000\n","Epoch 42/100\n","2/2 [==============================] - 2s 865ms/step - loss: 0.0546 - accuracy: 0.9796\n","Epoch 43/100\n","2/2 [==============================] - 2s 871ms/step - loss: 0.0176 - accuracy: 1.0000\n","Epoch 44/100\n","2/2 [==============================] - 2s 880ms/step - loss: 0.0195 - accuracy: 1.0000\n","Epoch 45/100\n","2/2 [==============================] - 2s 753ms/step - loss: 0.0157 - accuracy: 1.0000\n","Epoch 46/100\n","2/2 [==============================] - 1s 425ms/step - loss: 0.0216 - accuracy: 1.0000\n","Epoch 47/100\n","2/2 [==============================] - 1s 411ms/step - loss: 0.0214 - accuracy: 1.0000\n","Epoch 48/100\n","2/2 [==============================] - 1s 426ms/step - loss: 0.0165 - accuracy: 0.9898\n","Epoch 49/100\n","2/2 [==============================] - 1s 441ms/step - loss: 0.0085 - accuracy: 1.0000\n","Epoch 50/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0167 - accuracy: 0.9898\n","Epoch 51/100\n","2/2 [==============================] - 1s 463ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 52/100\n","2/2 [==============================] - 1s 428ms/step - loss: 0.0102 - accuracy: 1.0000\n","Epoch 53/100\n","2/2 [==============================] - 2s 822ms/step - loss: 0.0186 - accuracy: 0.9898\n","Epoch 54/100\n","2/2 [==============================] - 2s 859ms/step - loss: 0.0074 - accuracy: 1.0000\n","Epoch 55/100\n","2/2 [==============================] - 1s 445ms/step - loss: 0.0090 - accuracy: 1.0000\n","Epoch 56/100\n","2/2 [==============================] - 1s 474ms/step - loss: 0.0069 - accuracy: 1.0000\n","Epoch 57/100\n","2/2 [==============================] - 1s 430ms/step - loss: 0.0303 - accuracy: 0.9898\n","Epoch 58/100\n","2/2 [==============================] - 1s 454ms/step - loss: 0.0067 - accuracy: 1.0000\n","Epoch 59/100\n","2/2 [==============================] - 1s 413ms/step - loss: 0.0078 - accuracy: 1.0000\n","Epoch 60/100\n","2/2 [==============================] - 1s 436ms/step - loss: 0.0022 - accuracy: 1.0000\n","Epoch 61/100\n","2/2 [==============================] - 1s 417ms/step - loss: 0.0043 - accuracy: 1.0000\n","Epoch 62/100\n","2/2 [==============================] - 1s 453ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 63/100\n","2/2 [==============================] - 2s 834ms/step - loss: 0.0039 - accuracy: 1.0000\n","Epoch 64/100\n","2/2 [==============================] - 2s 847ms/step - loss: 0.0032 - accuracy: 1.0000\n","Epoch 65/100\n","2/2 [==============================] - 2s 512ms/step - loss: 0.0078 - accuracy: 1.0000\n","Epoch 66/100\n","2/2 [==============================] - 1s 445ms/step - loss: 0.0050 - accuracy: 1.0000\n","Epoch 67/100\n","2/2 [==============================] - 1s 423ms/step - loss: 0.0031 - accuracy: 1.0000\n","Epoch 68/100\n","2/2 [==============================] - 1s 456ms/step - loss: 0.0059 - accuracy: 1.0000\n","Epoch 69/100\n","2/2 [==============================] - 1s 453ms/step - loss: 0.0025 - accuracy: 1.0000\n","Epoch 70/100\n","2/2 [==============================] - 1s 451ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 71/100\n","2/2 [==============================] - 1s 416ms/step - loss: 0.0023 - accuracy: 1.0000\n","Epoch 72/100\n","2/2 [==============================] - 1s 422ms/step - loss: 2.5581e-04 - accuracy: 1.0000\n","Epoch 73/100\n","2/2 [==============================] - 1s 529ms/step - loss: 7.9667e-04 - accuracy: 1.0000\n","Epoch 74/100\n","2/2 [==============================] - 2s 743ms/step - loss: 0.0059 - accuracy: 1.0000\n","Epoch 75/100\n","2/2 [==============================] - 2s 799ms/step - loss: 0.0019 - accuracy: 1.0000\n","Epoch 76/100\n","2/2 [==============================] - 1s 447ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 77/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.0139 - accuracy: 0.9898\n","Epoch 78/100\n","2/2 [==============================] - 1s 438ms/step - loss: 0.0028 - accuracy: 1.0000\n","Epoch 79/100\n","2/2 [==============================] - 1s 427ms/step - loss: 0.0029 - accuracy: 1.0000\n","Epoch 80/100\n","2/2 [==============================] - 1s 448ms/step - loss: 0.0118 - accuracy: 1.0000\n","Epoch 81/100\n","2/2 [==============================] - 1s 410ms/step - loss: 0.0028 - accuracy: 1.0000\n","Epoch 82/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.0062 - accuracy: 1.0000\n","Epoch 83/100\n","2/2 [==============================] - 1s 432ms/step - loss: 0.0077 - accuracy: 1.0000\n","Epoch 84/100\n","2/2 [==============================] - 2s 747ms/step - loss: 0.0046 - accuracy: 1.0000\n","Epoch 85/100\n","2/2 [==============================] - 2s 820ms/step - loss: 0.0030 - accuracy: 1.0000\n","Epoch 86/100\n","2/2 [==============================] - 2s 523ms/step - loss: 0.0028 - accuracy: 1.0000\n","Epoch 87/100\n","2/2 [==============================] - 1s 420ms/step - loss: 0.0051 - accuracy: 1.0000\n","Epoch 88/100\n","2/2 [==============================] - 1s 414ms/step - loss: 0.0121 - accuracy: 0.9898\n","Epoch 89/100\n","2/2 [==============================] - 1s 412ms/step - loss: 0.0043 - accuracy: 1.0000\n","Epoch 90/100\n","2/2 [==============================] - 1s 447ms/step - loss: 0.0052 - accuracy: 1.0000\n","Epoch 91/100\n","2/2 [==============================] - 1s 432ms/step - loss: 0.0015 - accuracy: 1.0000\n","Epoch 92/100\n","2/2 [==============================] - 1s 474ms/step - loss: 5.6813e-04 - accuracy: 1.0000\n","Epoch 93/100\n","2/2 [==============================] - 1s 431ms/step - loss: 5.9182e-04 - accuracy: 1.0000\n","Epoch 94/100\n","2/2 [==============================] - 1s 421ms/step - loss: 6.6315e-04 - accuracy: 1.0000\n","Epoch 95/100\n","2/2 [==============================] - 2s 807ms/step - loss: 0.0019 - accuracy: 1.0000\n","Epoch 96/100\n","2/2 [==============================] - 2s 765ms/step - loss: 0.0016 - accuracy: 1.0000\n","Epoch 97/100\n","2/2 [==============================] - 2s 457ms/step - loss: 0.0013 - accuracy: 1.0000\n","Epoch 98/100\n","2/2 [==============================] - 1s 414ms/step - loss: 0.0085 - accuracy: 1.0000\n","Epoch 99/100\n","2/2 [==============================] - 1s 450ms/step - loss: 9.6706e-04 - accuracy: 1.0000\n","Epoch 100/100\n","2/2 [==============================] - 1s 414ms/step - loss: 8.9366e-04 - accuracy: 1.0000\n","6/6 [==============================] - 0s 34ms/step\n","Predict Classes [[7.16294511e-04 9.99283731e-01]\n"," [7.79765469e-05 9.99922037e-01]\n"," [1.07857988e-04 9.99892116e-01]\n"," [9.99971986e-01 2.80379372e-05]\n"," [2.91664898e-01 7.08335102e-01]\n"," [3.61288730e-05 9.99963880e-01]\n"," [2.07138373e-06 9.99997973e-01]\n"," [7.11813826e-08 9.99999881e-01]\n"," [1.51003732e-07 9.99999881e-01]\n"," [2.00525814e-04 9.99799550e-01]\n"," [4.94122796e-04 9.99505877e-01]\n"," [2.05845819e-04 9.99794185e-01]\n"," [1.04001806e-07 9.99999881e-01]\n"," [1.68517490e-05 9.99983191e-01]\n"," [1.70927557e-08 1.00000000e+00]\n"," [9.99875546e-01 1.24489918e-04]\n"," [1.00000000e+00 3.82661725e-09]\n"," [9.99999285e-01 7.74682178e-07]\n"," [9.99999881e-01 1.37335689e-07]\n"," [1.00000000e+00 6.40465597e-11]\n"," [1.00000000e+00 5.36531271e-11]\n"," [9.99999762e-01 1.89727203e-07]\n"," [6.18018239e-05 9.99938250e-01]\n"," [9.99998569e-01 1.41430473e-06]]\n","Evaluate on test data\n","test loss, test acc: [0.8550146222114563, 0.9166666865348816]\n","6/6 [==============================] - 0s 34ms/step\n","\n","Classification Report\n","\n","[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","(24,)\n","[1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0]\n","(24,)\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.89      0.89      0.89         9\n","     Class 1       0.93      0.93      0.93        15\n","\n","    accuracy                           0.92        24\n","   macro avg       0.91      0.91      0.91        24\n","weighted avg       0.92      0.92      0.92        24\n","\n","[[ 8  1]\n"," [ 1 14]]\n","TRAIN: [  0   1   2   3   5   8   9  10  11  12  13  14  15  16  17  18  19  20\n","  21  23  24  25  27  28  29  30  32  34  35  37  38  39  40  41  42  43\n","  45  46  47  49  50  51  52  55  56  57  58  59  62  63  64  65  66  67\n","  68  69  71  72  73  74  76  77  78  79  81  82  83  84  86  87  88  89\n","  90  91  92  93  94  95  96  97  98 101 103 104 105 107 108 110 111 112\n"," 113 114 115 116 117 118 119 120] TEST: [  4   6   7  22  26  31  33  36  44  48  53  54  60  61  70  75  80  85\n","  99 100 102 106 109 121]\n","Test label [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 78, 78, 32)        320       \n","                                                                 \n"," average_pooling2d (AverageP  (None, 39, 39, 32)       0         \n"," ooling2D)                                                       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 37, 37, 64)        18496     \n","                                                                 \n"," average_pooling2d_1 (Averag  (None, 18, 18, 64)       0         \n"," ePooling2D)                                                     \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n","                                                                 \n"," flatten (Flatten)           (None, 32768)             0         \n","                                                                 \n"," dense (Dense)               (None, 256)               8388864   \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                16448     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 8,498,114\n","Trainable params: 8,498,114\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train Labels [[0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]]\n","Epoch 1/100\n","2/2 [==============================] - 2s 434ms/step - loss: 0.6918 - accuracy: 0.4796\n","Epoch 2/100\n","2/2 [==============================] - 1s 453ms/step - loss: 0.6831 - accuracy: 0.5000\n","Epoch 3/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.6420 - accuracy: 0.6122\n","Epoch 4/100\n","2/2 [==============================] - 1s 464ms/step - loss: 0.6433 - accuracy: 0.5612\n","Epoch 5/100\n","2/2 [==============================] - 1s 420ms/step - loss: 0.6265 - accuracy: 0.6735\n","Epoch 6/100\n","2/2 [==============================] - 1s 435ms/step - loss: 0.5973 - accuracy: 0.7653\n","Epoch 7/100\n","2/2 [==============================] - 1s 487ms/step - loss: 0.5377 - accuracy: 0.6939\n","Epoch 8/100\n","2/2 [==============================] - 2s 820ms/step - loss: 0.5002 - accuracy: 0.6939\n","Epoch 9/100\n","2/2 [==============================] - 2s 786ms/step - loss: 0.5076 - accuracy: 0.7857\n","Epoch 10/100\n","2/2 [==============================] - 1s 450ms/step - loss: 0.3770 - accuracy: 0.8878\n","Epoch 11/100\n","2/2 [==============================] - 1s 456ms/step - loss: 0.3852 - accuracy: 0.8265\n","Epoch 12/100\n","2/2 [==============================] - 1s 439ms/step - loss: 0.3361 - accuracy: 0.8571\n","Epoch 13/100\n","2/2 [==============================] - 1s 460ms/step - loss: 0.2798 - accuracy: 0.8980\n","Epoch 14/100\n","2/2 [==============================] - 1s 428ms/step - loss: 0.3354 - accuracy: 0.8571\n","Epoch 15/100\n","2/2 [==============================] - 1s 414ms/step - loss: 0.2925 - accuracy: 0.8776\n","Epoch 16/100\n","2/2 [==============================] - 1s 461ms/step - loss: 0.2662 - accuracy: 0.9184\n","Epoch 17/100\n","2/2 [==============================] - 1s 455ms/step - loss: 0.2263 - accuracy: 0.9286\n","Epoch 18/100\n","2/2 [==============================] - 2s 797ms/step - loss: 0.2173 - accuracy: 0.9388\n","Epoch 19/100\n","2/2 [==============================] - 2s 806ms/step - loss: 0.1572 - accuracy: 0.9592\n","Epoch 20/100\n","2/2 [==============================] - 2s 622ms/step - loss: 0.1309 - accuracy: 0.9898\n","Epoch 21/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.1750 - accuracy: 0.9490\n","Epoch 22/100\n","2/2 [==============================] - 1s 417ms/step - loss: 0.1619 - accuracy: 0.9184\n","Epoch 23/100\n","2/2 [==============================] - 1s 466ms/step - loss: 0.1369 - accuracy: 0.9592\n","Epoch 24/100\n","2/2 [==============================] - 1s 430ms/step - loss: 0.1490 - accuracy: 0.9388\n","Epoch 25/100\n","2/2 [==============================] - 1s 451ms/step - loss: 0.1412 - accuracy: 0.9490\n","Epoch 26/100\n","2/2 [==============================] - 1s 438ms/step - loss: 0.2292 - accuracy: 0.9388\n","Epoch 27/100\n","2/2 [==============================] - 1s 436ms/step - loss: 0.3356 - accuracy: 0.9184\n","Epoch 28/100\n","2/2 [==============================] - 1s 432ms/step - loss: 0.1116 - accuracy: 0.9490\n","Epoch 29/100\n","2/2 [==============================] - 2s 868ms/step - loss: 0.3237 - accuracy: 0.8571\n","Epoch 30/100\n","2/2 [==============================] - 2s 795ms/step - loss: 0.1414 - accuracy: 0.9592\n","Epoch 31/100\n","2/2 [==============================] - 2s 442ms/step - loss: 0.1948 - accuracy: 0.8980\n","Epoch 32/100\n","2/2 [==============================] - 1s 420ms/step - loss: 0.1756 - accuracy: 0.9592\n","Epoch 33/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.1021 - accuracy: 0.9796\n","Epoch 34/100\n","2/2 [==============================] - 1s 447ms/step - loss: 0.1641 - accuracy: 0.9388\n","Epoch 35/100\n","2/2 [==============================] - 1s 451ms/step - loss: 0.0813 - accuracy: 0.9694\n","Epoch 36/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0755 - accuracy: 0.9694\n","Epoch 37/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0893 - accuracy: 0.9694\n","Epoch 38/100\n","2/2 [==============================] - 2s 740ms/step - loss: 0.0577 - accuracy: 0.9796\n","Epoch 39/100\n","2/2 [==============================] - 2s 831ms/step - loss: 0.0813 - accuracy: 0.9796\n","Epoch 40/100\n","2/2 [==============================] - 2s 851ms/step - loss: 0.0840 - accuracy: 0.9796\n","Epoch 41/100\n","2/2 [==============================] - 2s 842ms/step - loss: 0.0292 - accuracy: 0.9898\n","Epoch 42/100\n","2/2 [==============================] - 2s 665ms/step - loss: 0.0702 - accuracy: 0.9796\n","Epoch 43/100\n","2/2 [==============================] - 1s 427ms/step - loss: 0.0364 - accuracy: 0.9898\n","Epoch 44/100\n","2/2 [==============================] - 1s 424ms/step - loss: 0.0684 - accuracy: 0.9898\n","Epoch 45/100\n","2/2 [==============================] - 1s 428ms/step - loss: 0.0717 - accuracy: 0.9694\n","Epoch 46/100\n","2/2 [==============================] - 1s 443ms/step - loss: 0.0570 - accuracy: 0.9898\n","Epoch 47/100\n","2/2 [==============================] - 1s 430ms/step - loss: 0.0777 - accuracy: 0.9592\n","Epoch 48/100\n","2/2 [==============================] - 1s 463ms/step - loss: 0.0203 - accuracy: 1.0000\n","Epoch 49/100\n","2/2 [==============================] - 1s 467ms/step - loss: 0.0939 - accuracy: 0.9592\n","Epoch 50/100\n","2/2 [==============================] - 1s 424ms/step - loss: 0.0188 - accuracy: 1.0000\n","Epoch 51/100\n","2/2 [==============================] - 2s 826ms/step - loss: 0.0656 - accuracy: 0.9796\n","Epoch 52/100\n","2/2 [==============================] - 2s 814ms/step - loss: 0.0406 - accuracy: 0.9898\n","Epoch 53/100\n","2/2 [==============================] - 2s 564ms/step - loss: 0.0255 - accuracy: 1.0000\n","Epoch 54/100\n","2/2 [==============================] - 1s 449ms/step - loss: 0.0164 - accuracy: 1.0000\n","Epoch 55/100\n","2/2 [==============================] - 1s 429ms/step - loss: 0.0323 - accuracy: 0.9898\n","Epoch 56/100\n","2/2 [==============================] - 1s 449ms/step - loss: 0.0262 - accuracy: 0.9898\n","Epoch 57/100\n","2/2 [==============================] - 1s 441ms/step - loss: 0.0167 - accuracy: 1.0000\n","Epoch 58/100\n","2/2 [==============================] - 1s 430ms/step - loss: 0.0167 - accuracy: 1.0000\n","Epoch 59/100\n","2/2 [==============================] - 1s 461ms/step - loss: 0.0149 - accuracy: 1.0000\n","Epoch 60/100\n","2/2 [==============================] - 1s 450ms/step - loss: 0.0146 - accuracy: 1.0000\n","Epoch 61/100\n","2/2 [==============================] - 1s 457ms/step - loss: 0.0340 - accuracy: 0.9898\n","Epoch 62/100\n","2/2 [==============================] - 2s 837ms/step - loss: 0.0080 - accuracy: 1.0000\n","Epoch 63/100\n","2/2 [==============================] - 2s 815ms/step - loss: 0.0179 - accuracy: 1.0000\n","Epoch 64/100\n","2/2 [==============================] - 2s 437ms/step - loss: 0.0235 - accuracy: 1.0000\n","Epoch 65/100\n","2/2 [==============================] - 1s 430ms/step - loss: 0.0098 - accuracy: 1.0000\n","Epoch 66/100\n","2/2 [==============================] - 1s 462ms/step - loss: 0.0104 - accuracy: 1.0000\n","Epoch 67/100\n","2/2 [==============================] - 1s 464ms/step - loss: 0.0057 - accuracy: 1.0000\n","Epoch 68/100\n","2/2 [==============================] - 1s 445ms/step - loss: 0.0088 - accuracy: 1.0000\n","Epoch 69/100\n","2/2 [==============================] - 1s 426ms/step - loss: 0.0123 - accuracy: 1.0000\n","Epoch 70/100\n","2/2 [==============================] - 1s 445ms/step - loss: 0.0090 - accuracy: 1.0000\n","Epoch 71/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0031 - accuracy: 1.0000\n","Epoch 72/100\n","2/2 [==============================] - 1s 553ms/step - loss: 0.0106 - accuracy: 1.0000\n","Epoch 73/100\n","2/2 [==============================] - 2s 755ms/step - loss: 0.0032 - accuracy: 1.0000\n","Epoch 74/100\n","2/2 [==============================] - 2s 778ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 75/100\n","2/2 [==============================] - 1s 437ms/step - loss: 0.0012 - accuracy: 1.0000\n","Epoch 76/100\n","2/2 [==============================] - 1s 413ms/step - loss: 0.0059 - accuracy: 1.0000\n","Epoch 77/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.0028 - accuracy: 1.0000\n","Epoch 78/100\n","2/2 [==============================] - 1s 463ms/step - loss: 0.0070 - accuracy: 1.0000\n","Epoch 79/100\n","2/2 [==============================] - 1s 457ms/step - loss: 0.0023 - accuracy: 1.0000\n","Epoch 80/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0025 - accuracy: 1.0000\n","Epoch 81/100\n","2/2 [==============================] - 1s 450ms/step - loss: 0.0278 - accuracy: 0.9898\n","Epoch 82/100\n","2/2 [==============================] - 1s 432ms/step - loss: 0.0021 - accuracy: 1.0000\n","Epoch 83/100\n","2/2 [==============================] - 2s 849ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 84/100\n","2/2 [==============================] - 2s 833ms/step - loss: 0.0108 - accuracy: 1.0000\n","Epoch 85/100\n","2/2 [==============================] - 2s 535ms/step - loss: 0.0070 - accuracy: 1.0000\n","Epoch 86/100\n","2/2 [==============================] - 1s 425ms/step - loss: 0.0051 - accuracy: 1.0000\n","Epoch 87/100\n","2/2 [==============================] - 1s 430ms/step - loss: 0.0015 - accuracy: 1.0000\n","Epoch 88/100\n","2/2 [==============================] - 1s 439ms/step - loss: 0.0039 - accuracy: 1.0000\n","Epoch 89/100\n","2/2 [==============================] - 1s 453ms/step - loss: 0.0048 - accuracy: 1.0000\n","Epoch 90/100\n","2/2 [==============================] - 1s 461ms/step - loss: 0.0068 - accuracy: 1.0000\n","Epoch 91/100\n","2/2 [==============================] - 1s 455ms/step - loss: 0.0039 - accuracy: 1.0000\n","Epoch 92/100\n","2/2 [==============================] - 1s 438ms/step - loss: 0.0027 - accuracy: 1.0000\n","Epoch 93/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0049 - accuracy: 1.0000\n","Epoch 94/100\n","2/2 [==============================] - 2s 829ms/step - loss: 0.0040 - accuracy: 1.0000\n","Epoch 95/100\n","2/2 [==============================] - 2s 801ms/step - loss: 0.0029 - accuracy: 1.0000\n","Epoch 96/100\n","2/2 [==============================] - 1s 424ms/step - loss: 0.0075 - accuracy: 1.0000\n","Epoch 97/100\n","2/2 [==============================] - 1s 418ms/step - loss: 0.0030 - accuracy: 1.0000\n","Epoch 98/100\n","2/2 [==============================] - 1s 441ms/step - loss: 0.0024 - accuracy: 1.0000\n","Epoch 99/100\n","2/2 [==============================] - 1s 418ms/step - loss: 0.0050 - accuracy: 1.0000\n","Epoch 100/100\n","2/2 [==============================] - 1s 440ms/step - loss: 0.0014 - accuracy: 1.0000\n","6/6 [==============================] - 0s 22ms/step\n","Predict Classes [[6.25258035e-05 9.99937415e-01]\n"," [1.08069309e-03 9.98919368e-01]\n"," [1.20912009e-04 9.99879122e-01]\n"," [1.29882628e-05 9.99987006e-01]\n"," [3.34423639e-05 9.99966502e-01]\n"," [5.47209347e-05 9.99945283e-01]\n"," [4.11624842e-06 9.99995828e-01]\n"," [7.48466555e-05 9.99925137e-01]\n"," [1.06491757e-06 9.99998927e-01]\n"," [6.12018630e-06 9.99993920e-01]\n"," [1.05189174e-04 9.99894857e-01]\n"," [2.59840306e-07 9.99999762e-01]\n"," [2.48350744e-07 9.99999762e-01]\n"," [1.59759729e-05 9.99984026e-01]\n"," [9.99972224e-01 2.77518884e-05]\n"," [1.00000000e+00 1.99430943e-08]\n"," [9.99999881e-01 1.37180578e-07]\n"," [9.99999166e-01 8.01122212e-07]\n"," [9.93781388e-01 6.21862244e-03]\n"," [1.00000000e+00 1.10914892e-13]\n"," [8.64080012e-01 1.35919988e-01]\n"," [1.00000000e+00 2.30438336e-13]\n"," [1.00000000e+00 2.49067984e-15]\n"," [1.00000000e+00 9.83166604e-11]]\n","Evaluate on test data\n","test loss, test acc: [0.006413764785975218, 1.0]\n","6/6 [==============================] - 0s 23ms/step\n","\n","Classification Report\n","\n","[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","(24,)\n","[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n","(24,)\n","              precision    recall  f1-score   support\n","\n","     Class 0       1.00      1.00      1.00        10\n","     Class 1       1.00      1.00      1.00        14\n","\n","    accuracy                           1.00        24\n","   macro avg       1.00      1.00      1.00        24\n","weighted avg       1.00      1.00      1.00        24\n","\n","[[10  0]\n"," [ 0 14]]\n","TRAIN: [  0   2   3   4   5   6   7   8  10  11  12  14  15  18  20  21  22  23\n","  24  25  26  27  29  31  32  33  35  36  37  38  39  41  42  44  45  47\n","  48  50  51  52  53  54  55  57  58  59  60  61  62  63  64  65  66  67\n","  69  70  71  72  73  74  75  77  78  80  81  82  83  84  85  86  88  90\n","  91  92  93  94  96  97  98  99 100 101 102 103 106 107 108 109 111 112\n"," 113 114 115 116 118 119 120 121] TEST: [  1   9  13  16  17  19  28  30  34  40  43  46  49  56  68  76  79  87\n","  89  95 104 105 110 117]\n","Test label [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 78, 78, 32)        320       \n","                                                                 \n"," average_pooling2d (AverageP  (None, 39, 39, 32)       0         \n"," ooling2D)                                                       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 37, 37, 64)        18496     \n","                                                                 \n"," average_pooling2d_1 (Averag  (None, 18, 18, 64)       0         \n"," ePooling2D)                                                     \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n","                                                                 \n"," flatten (Flatten)           (None, 32768)             0         \n","                                                                 \n"," dense (Dense)               (None, 256)               8388864   \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                16448     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 8,498,114\n","Trainable params: 8,498,114\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train Labels [[0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]]\n","Epoch 1/100\n","2/2 [==============================] - 2s 445ms/step - loss: 0.6898 - accuracy: 0.4796\n","Epoch 2/100\n","2/2 [==============================] - 2s 779ms/step - loss: 0.6877 - accuracy: 0.5510\n","Epoch 3/100\n","2/2 [==============================] - 2s 829ms/step - loss: 0.6343 - accuracy: 0.6633\n","Epoch 4/100\n","2/2 [==============================] - 2s 495ms/step - loss: 0.6385 - accuracy: 0.5918\n","Epoch 5/100\n","2/2 [==============================] - 1s 417ms/step - loss: 0.6064 - accuracy: 0.6939\n","Epoch 6/100\n","2/2 [==============================] - 1s 446ms/step - loss: 0.5455 - accuracy: 0.7449\n","Epoch 7/100\n","2/2 [==============================] - 1s 426ms/step - loss: 0.4612 - accuracy: 0.7959\n","Epoch 8/100\n","2/2 [==============================] - 1s 425ms/step - loss: 0.3604 - accuracy: 0.8673\n","Epoch 9/100\n","2/2 [==============================] - 1s 427ms/step - loss: 0.4014 - accuracy: 0.8469\n","Epoch 10/100\n","2/2 [==============================] - 1s 442ms/step - loss: 0.5549 - accuracy: 0.7857\n","Epoch 11/100\n","2/2 [==============================] - 1s 459ms/step - loss: 0.2684 - accuracy: 0.8673\n","Epoch 12/100\n","2/2 [==============================] - 1s 465ms/step - loss: 0.3511 - accuracy: 0.8367\n","Epoch 13/100\n","2/2 [==============================] - 2s 808ms/step - loss: 0.2631 - accuracy: 0.9082\n","Epoch 14/100\n","2/2 [==============================] - 2s 775ms/step - loss: 0.3221 - accuracy: 0.8776\n","Epoch 15/100\n","2/2 [==============================] - 1s 454ms/step - loss: 0.3165 - accuracy: 0.8776\n","Epoch 16/100\n","2/2 [==============================] - 1s 431ms/step - loss: 0.2456 - accuracy: 0.9286\n","Epoch 17/100\n","2/2 [==============================] - 1s 409ms/step - loss: 0.2544 - accuracy: 0.8571\n","Epoch 18/100\n","2/2 [==============================] - 1s 421ms/step - loss: 0.2319 - accuracy: 0.9184\n","Epoch 19/100\n","2/2 [==============================] - 1s 436ms/step - loss: 0.2992 - accuracy: 0.8980\n","Epoch 20/100\n","2/2 [==============================] - 1s 456ms/step - loss: 0.2052 - accuracy: 0.8878\n","Epoch 21/100\n","2/2 [==============================] - 1s 443ms/step - loss: 0.2321 - accuracy: 0.8878\n","Epoch 22/100\n","2/2 [==============================] - 1s 448ms/step - loss: 0.1395 - accuracy: 0.9592\n","Epoch 23/100\n","2/2 [==============================] - 2s 835ms/step - loss: 0.1943 - accuracy: 0.9184\n","Epoch 24/100\n","2/2 [==============================] - 2s 787ms/step - loss: 0.1856 - accuracy: 0.9286\n","Epoch 25/100\n","2/2 [==============================] - 2s 616ms/step - loss: 0.1291 - accuracy: 0.9796\n","Epoch 26/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.1679 - accuracy: 0.9184\n","Epoch 27/100\n","2/2 [==============================] - 1s 424ms/step - loss: 0.1124 - accuracy: 0.9490\n","Epoch 28/100\n","2/2 [==============================] - 1s 459ms/step - loss: 0.1372 - accuracy: 0.9592\n","Epoch 29/100\n","2/2 [==============================] - 1s 445ms/step - loss: 0.1764 - accuracy: 0.9082\n","Epoch 30/100\n","2/2 [==============================] - 1s 420ms/step - loss: 0.1452 - accuracy: 0.9388\n","Epoch 31/100\n","2/2 [==============================] - 1s 439ms/step - loss: 0.1206 - accuracy: 0.9388\n","Epoch 32/100\n","2/2 [==============================] - 1s 436ms/step - loss: 0.0842 - accuracy: 0.9694\n","Epoch 33/100\n","2/2 [==============================] - 1s 472ms/step - loss: 0.0947 - accuracy: 0.9694\n","Epoch 34/100\n","2/2 [==============================] - 2s 828ms/step - loss: 0.0582 - accuracy: 0.9898\n","Epoch 35/100\n","2/2 [==============================] - 2s 825ms/step - loss: 0.0793 - accuracy: 0.9694\n","Epoch 36/100\n","2/2 [==============================] - 2s 417ms/step - loss: 0.0480 - accuracy: 0.9898\n","Epoch 37/100\n","2/2 [==============================] - 1s 487ms/step - loss: 0.0627 - accuracy: 0.9898\n","Epoch 38/100\n","2/2 [==============================] - 1s 430ms/step - loss: 0.0469 - accuracy: 0.9796\n","Epoch 39/100\n","2/2 [==============================] - 1s 456ms/step - loss: 0.0385 - accuracy: 0.9898\n","Epoch 40/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0277 - accuracy: 1.0000\n","Epoch 41/100\n","2/2 [==============================] - 1s 444ms/step - loss: 0.0211 - accuracy: 1.0000\n","Epoch 42/100\n","2/2 [==============================] - 1s 454ms/step - loss: 0.0339 - accuracy: 0.9898\n","Epoch 43/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.0246 - accuracy: 0.9898\n","Epoch 44/100\n","2/2 [==============================] - 1s 705ms/step - loss: 0.0347 - accuracy: 0.9898\n","Epoch 45/100\n","2/2 [==============================] - 2s 869ms/step - loss: 0.0225 - accuracy: 1.0000\n","Epoch 46/100\n","2/2 [==============================] - 2s 641ms/step - loss: 0.0216 - accuracy: 1.0000\n","Epoch 47/100\n","2/2 [==============================] - 1s 440ms/step - loss: 0.0375 - accuracy: 0.9898\n","Epoch 48/100\n","2/2 [==============================] - 1s 441ms/step - loss: 0.0215 - accuracy: 1.0000\n","Epoch 49/100\n","2/2 [==============================] - 1s 412ms/step - loss: 0.0137 - accuracy: 1.0000\n","Epoch 50/100\n","2/2 [==============================] - 1s 443ms/step - loss: 0.0118 - accuracy: 1.0000\n","Epoch 51/100\n","2/2 [==============================] - 1s 453ms/step - loss: 0.0049 - accuracy: 1.0000\n","Epoch 52/100\n","2/2 [==============================] - 1s 455ms/step - loss: 0.0307 - accuracy: 0.9898\n","Epoch 53/100\n","2/2 [==============================] - 1s 433ms/step - loss: 0.0086 - accuracy: 1.0000\n","Epoch 54/100\n","2/2 [==============================] - 1s 424ms/step - loss: 0.0212 - accuracy: 0.9898\n","Epoch 55/100\n","2/2 [==============================] - 2s 810ms/step - loss: 0.0052 - accuracy: 1.0000\n","Epoch 56/100\n","2/2 [==============================] - 2s 788ms/step - loss: 0.0146 - accuracy: 0.9898\n","Epoch 57/100\n","2/2 [==============================] - 2s 455ms/step - loss: 0.0024 - accuracy: 1.0000\n","Epoch 58/100\n","2/2 [==============================] - 1s 463ms/step - loss: 0.0187 - accuracy: 1.0000\n","Epoch 59/100\n","2/2 [==============================] - 1s 420ms/step - loss: 0.0136 - accuracy: 1.0000\n","Epoch 60/100\n","2/2 [==============================] - 1s 418ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 61/100\n","2/2 [==============================] - 1s 427ms/step - loss: 0.0109 - accuracy: 1.0000\n","Epoch 62/100\n","2/2 [==============================] - 1s 450ms/step - loss: 0.0050 - accuracy: 1.0000\n","Epoch 63/100\n","2/2 [==============================] - 1s 471ms/step - loss: 0.0050 - accuracy: 1.0000\n","Epoch 64/100\n","2/2 [==============================] - 1s 451ms/step - loss: 0.0023 - accuracy: 1.0000\n","Epoch 65/100\n","2/2 [==============================] - 1s 535ms/step - loss: 0.0047 - accuracy: 1.0000\n","Epoch 66/100\n","2/2 [==============================] - 2s 758ms/step - loss: 0.0032 - accuracy: 1.0000\n","Epoch 67/100\n","2/2 [==============================] - 2s 811ms/step - loss: 0.0230 - accuracy: 0.9898\n","Epoch 68/100\n","2/2 [==============================] - 1s 447ms/step - loss: 0.0040 - accuracy: 1.0000\n","Epoch 69/100\n","2/2 [==============================] - 1s 442ms/step - loss: 0.0035 - accuracy: 1.0000\n","Epoch 70/100\n","2/2 [==============================] - 1s 433ms/step - loss: 0.0075 - accuracy: 1.0000\n","Epoch 71/100\n","2/2 [==============================] - 1s 428ms/step - loss: 0.0096 - accuracy: 1.0000\n","Epoch 72/100\n","2/2 [==============================] - 1s 452ms/step - loss: 0.0035 - accuracy: 1.0000\n","Epoch 73/100\n","2/2 [==============================] - 1s 462ms/step - loss: 0.0138 - accuracy: 0.9898\n","Epoch 74/100\n","2/2 [==============================] - 1s 438ms/step - loss: 0.0104 - accuracy: 1.0000\n","Epoch 75/100\n","2/2 [==============================] - 1s 449ms/step - loss: 0.0077 - accuracy: 1.0000\n","Epoch 76/100\n","2/2 [==============================] - 2s 822ms/step - loss: 0.0325 - accuracy: 0.9898\n","Epoch 77/100\n","2/2 [==============================] - 2s 845ms/step - loss: 0.0013 - accuracy: 1.0000\n","Epoch 78/100\n","2/2 [==============================] - 2s 843ms/step - loss: 0.0312 - accuracy: 0.9796\n","Epoch 79/100\n","2/2 [==============================] - 2s 863ms/step - loss: 0.0034 - accuracy: 1.0000\n","Epoch 80/100\n","2/2 [==============================] - 2s 465ms/step - loss: 0.1299 - accuracy: 0.9592\n","Epoch 81/100\n","2/2 [==============================] - 1s 456ms/step - loss: 0.1044 - accuracy: 0.9592\n","Epoch 82/100\n","2/2 [==============================] - 1s 436ms/step - loss: 0.0142 - accuracy: 1.0000\n","Epoch 83/100\n","2/2 [==============================] - 1s 421ms/step - loss: 0.0480 - accuracy: 0.9796\n","Epoch 84/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.1098 - accuracy: 0.9490\n","Epoch 85/100\n","2/2 [==============================] - 1s 449ms/step - loss: 0.0458 - accuracy: 0.9694\n","Epoch 86/100\n","2/2 [==============================] - 1s 457ms/step - loss: 0.0426 - accuracy: 0.9898\n","Epoch 87/100\n","2/2 [==============================] - 1s 443ms/step - loss: 0.0070 - accuracy: 1.0000\n","Epoch 88/100\n","2/2 [==============================] - 1s 533ms/step - loss: 0.0243 - accuracy: 0.9898\n","Epoch 89/100\n","2/2 [==============================] - 2s 793ms/step - loss: 0.0404 - accuracy: 0.9694\n","Epoch 90/100\n","2/2 [==============================] - 2s 832ms/step - loss: 0.0203 - accuracy: 0.9898\n","Epoch 91/100\n","2/2 [==============================] - 1s 417ms/step - loss: 0.0299 - accuracy: 1.0000\n","Epoch 92/100\n","2/2 [==============================] - 1s 465ms/step - loss: 0.0136 - accuracy: 1.0000\n","Epoch 93/100\n","2/2 [==============================] - 1s 429ms/step - loss: 0.0063 - accuracy: 1.0000\n","Epoch 94/100\n","2/2 [==============================] - 1s 424ms/step - loss: 0.0060 - accuracy: 1.0000\n","Epoch 95/100\n","2/2 [==============================] - 1s 422ms/step - loss: 0.0072 - accuracy: 1.0000\n","Epoch 96/100\n","2/2 [==============================] - 1s 427ms/step - loss: 0.0058 - accuracy: 1.0000\n","Epoch 97/100\n","2/2 [==============================] - 1s 442ms/step - loss: 0.0055 - accuracy: 1.0000\n","Epoch 98/100\n","2/2 [==============================] - 1s 438ms/step - loss: 0.0050 - accuracy: 1.0000\n","Epoch 99/100\n","2/2 [==============================] - 2s 813ms/step - loss: 0.0076 - accuracy: 1.0000\n","Epoch 100/100\n","2/2 [==============================] - 2s 784ms/step - loss: 0.0045 - accuracy: 1.0000\n","6/6 [==============================] - 0s 25ms/step\n","Predict Classes [[7.1613518e-05 9.9992836e-01]\n"," [3.3328131e-05 9.9996662e-01]\n"," [9.9200761e-01 7.9923328e-03]\n"," [2.6083991e-04 9.9973911e-01]\n"," [1.7638115e-05 9.9998236e-01]\n"," [3.1015673e-03 9.9689841e-01]\n"," [2.9862212e-02 9.7013783e-01]\n"," [3.8970876e-03 9.9610293e-01]\n"," [1.2702111e-04 9.9987292e-01]\n"," [1.5286172e-05 9.9998474e-01]\n"," [3.2302958e-03 9.9676967e-01]\n"," [5.7546671e-08 1.0000000e+00]\n"," [5.0126004e-04 9.9949872e-01]\n"," [3.0600570e-06 9.9999690e-01]\n"," [9.9894017e-01 1.0598293e-03]\n"," [1.0000000e+00 1.0122927e-09]\n"," [9.7493625e-01 2.5063697e-02]\n"," [9.9999928e-01 6.5914554e-07]\n"," [9.9983561e-01 1.6440509e-04]\n"," [9.9869651e-01 1.3035404e-03]\n"," [9.9814832e-01 1.8517309e-03]\n"," [9.7308314e-01 2.6916869e-02]\n"," [9.9786997e-01 2.1300572e-03]\n"," [1.0000000e+00 2.2058695e-09]]\n","Evaluate on test data\n","test loss, test acc: [0.20541882514953613, 0.9583333134651184]\n","6/6 [==============================] - 0s 24ms/step\n","\n","Classification Report\n","\n","[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","(24,)\n","[1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n","(24,)\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.91      1.00      0.95        10\n","     Class 1       1.00      0.93      0.96        14\n","\n","    accuracy                           0.96        24\n","   macro avg       0.95      0.96      0.96        24\n","weighted avg       0.96      0.96      0.96        24\n","\n","[[10  0]\n"," [ 1 13]]\n","[0.95999998 0.92000002 0.91666669 1.         0.95833331]\n","Average is: 0.9509999990463257\n"]}],"source":["splits = 5\n","accuracy = np.zeros(splits)\n","index = 0;\n","kf = KFold(n_splits=splits, shuffle=True)\n","for train_index, test_index in kf.split(Data):\n","  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n","  print(\"Test label\",Label[test_index] )\n","  keras.backend.clear_session()\n","  model = create_model()\n","  accuracy[index] = train_and_evaluate__model(model, Data[train_index], to_categorical(Label[train_index]), Data[test_index], to_categorical(Label[test_index]))\n","  from sklearn.metrics import classification_report\n","  from sklearn.metrics import confusion_matrix\n","  prediticted_classes = model.predict(Data[test_index], batch_size=4)\n","  print('\\nClassification Report\\n')\n","  text_labels = Label[test_index]\n","  print(text_labels)\n","  print(text_labels.shape)\n","  preditiction = np.argmax(prediticted_classes, axis=1)\n","  print(preditiction)\n","  print(preditiction.shape)\n","  print(classification_report(text_labels, preditiction, target_names=['Class 0', 'Class 1']))\n","  print(confusion_matrix(text_labels, preditiction))\n","  index+=1\n","print(accuracy)\n","print(\"Average is:\", sum(accuracy)/5.0)\n"]},{"cell_type":"markdown","metadata":{"id":"xdFkvO-f_I_4"},"source":["*Clear* Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOcHXmV2_LiF"},"outputs":[],"source":["keras.backend.clear_session()"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ph_mhVGi_nKt9ocVoM1cvDpQ29XjwOCe","timestamp":1686082778234},{"file_id":"1psbr9beYK0rVjIvuYteoyDpyxw76XJTG","timestamp":1614757526673}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}